import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp
import functools

def apply_mask(weights, mask):
    all_keys = weights.keys()
    target_keys = mask.keys()
    remain_keys = list(set(all_keys) - set(target_keys))
    w_sparse = {k: mask[k] * weights[k] for k in target_keys}
    w_sparse.update({k: weights[k] for k in remain_keys})
    return w_sparse

def calc_gradient_penalty(model, real_cat, fake_cat, gp_lambda=10):
        #random alpha(between 0 and 1) for each input batch to discriminator
        alpha = tf.random.uniform([real_cat.shape[0] // model.pac, 1, 1], 0., 1.)
        alpha = tf.tile(alpha, tf.constant([1, model.pac, real_cat.shape[1]], tf.int32))
        alpha = tf.reshape(alpha, [-1, real_cat.shape[1]])
        
        interpolates = alpha * real_cat + ((1 - alpha) * fake_cat)
        pacdim = model.pac * real_cat.shape[1]
        interpolates_disc = tf.reshape(interpolates,[-1, pacdim])
        
        weights_disc = model.construct_weights_disc(False)


        with tf.GradientTape() as tape:
            tape.watch(interpolates_disc)
            pred_interpolates = model.forward_pass_disc(weights_disc, interpolates_disc)

        gradients = tape.gradient(pred_interpolates, interpolates_disc)

        gradients_view = tf.norm(tf.reshape(gradients, [-1, model.pac * real_cat.shape[1]]), axis=1) - 1
        gradient_penalty = tf.reduce_mean(tf.square(gradients_view)) * gp_lambda
        return gradient_penalty

def gumbel_softmax(logits, tau=1.0, hard=False, dim=-1):
        """Samples from the Gumbel-Softmax distribution
        :cite:`maddison2016concrete`, :cite:`jang2016categorical` and
        optionally discretizes.
        Parameters
        ----------
        logits: tf.Tensor
            Un-normalized log probabilities.
        tau: float, default=1.0
            Non-negative scalar temperature.
        hard: bool, default=False
            If ``True``, the returned samples will be discretized as
            one-hot vectors, but will be differentiated as soft samples.
        dim: int, default=1
            The dimension along which softmax will be computed.
        Returns
        -------
        tf.Tensor
            Sampled tensor of same shape as ``logits`` from the
            Gumbel-Softmax distribution. If ``hard=True``, the returned samples
            will be one-hot, otherwise they will be probability distributions
            that sum to 1 across ``dim``.
        """

        gumbel_dist = tfp.distributions.Gumbel(loc=0, scale=1)
        gumbels = gumbel_dist.sample(tf.shape(logits))
        gumbels = (logits + gumbels) / tau
        output = tf.nn.softmax(gumbels, dim)

        if hard:
            index = tf.math.reduce_max(output, 1, keepdims=True)
            output_hard = tf.cast(tf.equal(output, index), output.dtype)
            output = tf.stop_gradient(output_hard - output) + output
        return output

def apply_activate(transformer, data):
        """Apply proper activation function to the output of the generator."""
        data_t = []
        st = 0
        for column_info in transformer.output_info_list:
            for span_info in column_info:
                if span_info.activation_fn == 'tanh':
                    ed = st + span_info.dim
                    data_t.append(tf.math.tanh(data[:, st:ed]))
                    st = ed
                elif span_info.activation_fn == 'softmax':
                    ed = st + span_info.dim
                    transformed = gumbel_softmax(data[:, st:ed], tau=0.2)
                    data_t.append(transformed)
                    st = ed
                else:
                    raise ValueError(f'Unexpected activation function {span_info.activation_fn}.')

        return tf.concat(data_t, axis=1)


def cross_entropy_loss(data, c, m, output_info):
        """Compute the cross entropy loss on the fixed discrete column."""
        loss = []
        st = 0
        st_c = 0
        for column_info in output_info:
            for span_info in column_info:
                if len(column_info) != 1 or span_info.activation_fn != 'softmax':
                    # not discrete column
                    st += span_info.dim
                else:
                    ed = st + span_info.dim
                    ed_c = st_c + span_info.dim
                    #c is the conditional vector
                    labels=c[:, st_c:ed_c]
                    #data is generated by generator
                    logits=data[:, st:ed]
                    tmp = tf.nn.softmax_cross_entropy_with_logits(
                        labels,
                        logits)
                    loss.append(tmp)
                    st = ed
                    st_c = ed_c

        loss = tf.stack(loss, axis=1)
        #we are interested in the loss for the feature that is conditioned on 
        m1 = tf.cast(m, dtype=tf.float32)
        return tf.reduce_mean(loss * m1)

def SNIP_ctgan(model, batchsize, target_sparsity, discriminator_lr=2e-4,discriminator_decay=1e-6):
    
    discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate = discriminator_lr, beta_1=0.5, beta_2=0.9, decay = discriminator_decay)

    weights_gen = model.construct_weights_gen(False)
    
    embed_dim = model._embedding_dim
    mean = tf.zeros(shape=(batchsize, embed_dim), dtype=tf.float32)
    std = mean + 1

    fakez = tf.random.normal(shape=(batchsize, embed_dim), mean=mean, stddev=std)
    condvec = model.sampler.sample_condvec(batchsize)

    if condvec is None:
        c1, m1, col, opt = None, None, None, None
        real_data = model.sampler.sample_data(batchsize, col, opt)
    else:
        c1, m1, col, opt = condvec
        c1 = tf.convert_to_tensor(np.array(c1))
        c1 = tf.cast(c1, dtype=tf.float32)
        m1 = tf.convert_to_tensor(np.array(m1))
        m1 = tf.cast(m1, dtype=tf.int32)
        #fakez is fed to generator
        fakez = tf.concat([fakez, c1], axis=1)
        perm = np.arange(batchsize)
        np.random.shuffle(perm)
        real_data = model.sampler.sample_data(batchsize, col[perm], opt[perm])
        c2 = tf.gather(c1, indices=perm)
                        

    fake_data = model.forward_pass_gen(weights_gen, fakez)
    fakeact = apply_activate(model.transformer, fake_data)
    
    #fake_cat and real_cat are ready to be packed
    if c1 is not None:
        fake_cat = tf.concat([fakeact, c1], axis=1)
        real_cat = tf.concat([real_data, c2], axis=1)
    else:
        fake_cat = fakeact
        real_cat = real

    # reshape the data for packed discriminator
    pacdim =  (model.transformer.output_dimensions + model.sampler.dim_cond_vec()) * model.pac
    fake_cat_disc = tf.reshape(fake_cat,[-1, pacdim])
    real_cat_disc = tf.reshape(real_cat, [-1, pacdim])

    weights_disc = model.construct_weights_disc(True)
    
    prn_keys_disc = []
    for key in weights_disc.keys():
        prn_keys_disc.append(key)

    prn_keys_gen = []
    for key in weights_gen.keys():
        prn_keys_gen.append(key)

    mask_init_disc = {k:tf.Variable(tf.ones(weights_disc[k].shape), trainable=False, dtype=tf.float32) for k in prn_keys_disc}
    mask_init_gen = {k:tf.Variable(tf.ones(weights_gen[k].shape), trainable=False, dtype=tf.float32) for k in prn_keys_gen}

    discriminator = model.make_layers_disc(weights_disc, mask_init_disc)
    #the gradients of discriminator loss function with respect to the discriminator's weights
    with tf.GradientTape() as tape_disc1:
        logits_real = discriminator(real_cat_disc, training=True)
        logits_fake = discriminator(fake_cat_disc, training=True)
        pen = calc_gradient_penalty(model, real_cat, fake_cat, 10)
        loss_d = -(tf.reduce_mean(logits_real) - tf.reduce_mean(logits_fake)) + pen
    grads_discriminator = tape_disc1.gradient(loss_d, discriminator.trainable_variables)

    #the gradients of discriminator loss function with respect to mask parameters 
    with tf.GradientTape() as tape_disc:
        tape_disc.watch(mask_init_disc)
        w_mask_disc = apply_mask(weights_disc, mask_init_disc)
        logits_real = model.forward_pass_disc(w_mask_disc, real_cat_disc)
        logits_fake = model.forward_pass_disc(w_mask_disc, fake_cat_disc)
        pen = calc_gradient_penalty(model, real_cat, fake_cat, 10)
        loss_d = -(tf.reduce_mean(logits_real) - tf.reduce_mean(logits_fake)) + pen
    grads_disc = tape_disc.gradient(loss_d, [mask_init_disc[k] for k in prn_keys_disc])
    
    #new random data to be fed to generator, to compute the gradients of generator loss function with respect to the mask
    fakez = tf.random.normal(shape=(batchsize, embed_dim), mean=mean, stddev=std)
    condvec = model.sampler.sample_condvec(batchsize)

    if condvec is not None:
       c1, m1, col, opt = condvec
       fakez = tf.concat([fakez, c1], axis=1)

    with tf.GradientTape() as tape_gen:
        tape_gen.watch(mask_init_gen)
        w_mask_gen = apply_mask(weights_gen, mask_init_gen)
        fake_data = model.forward_pass_gen(w_mask_gen, fakez)
        fakeact = apply_activate(model.transformer, fake_data)
        discriminator_optimizer.apply_gradients(zip(grads_discriminator, discriminator.trainable_weights))
        if condvec is not None:
            fake_temp = tf.concat([fakeact, c1], axis=1)
            y_fake = discriminator(tf.reshape(fake_temp,[-1, pacdim]))
            output_info = model.transformer.output_info_list
            cross_entropy = cross_entropy_loss(fake_data, c1, m1, output_info)
        else: 
            y_fake = discriminator(tf.reshape(fakeact,[-1, pacdim]))
            cross_entropy = 0

        loss_g = -tf.reduce_mean(y_fake) + cross_entropy
    grads_gen = tape_gen.gradient(loss_g, [mask_init_gen[k] for k in prn_keys_gen])

    gradients_disc = dict(zip(prn_keys_disc, grads_disc))

    gradients_abs_disc = {k: tf.abs(v) for k, v in gradients_disc.items()}
    grad_key_disc = gradients_abs_disc.keys()

    grad_shape_disc = {k: gradients_abs_disc[k].shape.as_list() for k in grad_key_disc}
    #grad_shape={'w1': [11, 11, 3, 96],'w2': [5, 5, 96, 256],'w3': [3, 3, 256, 384],'w4': [3, 3, 384, 384],'w5': [3, 3, 384, 256],'w6': [256, 1024],'w7': [1024, 1024],'w8': [1024, 10],
    #'b1': [96],'b2': [256],'b3': [384],'b4': [384],'b5': [256],'b6': [1024],'b7': [1024],'b8': [10]}
    split_sizes_disc = []
    for key in grad_key_disc:
       split_sizes_disc.append(functools.reduce(lambda x, y: x*y, grad_shape_disc[key]))   
    grad_v_disc = tf.concat([tf.reshape(gradients_abs_disc[k], [-1]) for k in grad_key_disc], axis=0) 
    normalgrad_v_disc = tf.divide(grad_v_disc, tf.reduce_sum(grad_v_disc))
    num_params_disc = normalgrad_v_disc.shape.as_list()[0]
    kappa_disc = int(round(num_params_disc * (1. - target_sparsity)))   
    topk_disc, ind_disc = tf.nn.top_k(normalgrad_v_disc, k=kappa_disc, sorted=True)
    sp_mask_disc = tf.SparseTensor(dense_shape=normalgrad_v_disc.shape.as_list(), values=tf.ones_like(ind_disc, dtype=tf.float32).numpy(), indices=np.expand_dims(ind_disc.numpy(), 1))
    mask_v_disc = tf.sparse.to_dense(sp_mask_disc, validate_indices=False)
    #restore mask_v as dictionary of weights
    v_splits_disc = tf.split(mask_v_disc, num_or_size_splits=split_sizes_disc)
    mask_restore_disc = {}
    for i, key in enumerate(grad_key_disc):
       mask_restore_disc.update({key: tf.reshape(v_splits_disc[i], grad_shape_disc[key])})
    final_mask_disc = mask_restore_disc
    final_w_disc = apply_mask(weights_disc, final_mask_disc)


    gradients_gen = dict(zip(prn_keys_gen, grads_gen))

    gradients_abs_gen = {k: tf.abs(v) for k, v in gradients_gen.items()}
    grad_key_gen = gradients_abs_gen.keys()

    grad_shape_gen = {k: gradients_abs_gen[k].shape.as_list() for k in grad_key_gen}
    #grad_shape={'w1': [11, 11, 3, 96],'w2': [5, 5, 96, 256],'w3': [3, 3, 256, 384],'w4': [3, 3, 384, 384],'w5': [3, 3, 384, 256],'w6': [256, 1024],'w7': [1024, 1024],'w8': [1024, 10],
    #'b1': [96],'b2': [256],'b3': [384],'b4': [384],'b5': [256],'b6': [1024],'b7': [1024],'b8': [10]}
    split_sizes_gen = []
    for key in grad_key_gen:
       split_sizes_gen.append(functools.reduce(lambda x, y: x*y, grad_shape_gen[key]))   
    grad_v_gen = tf.concat([tf.reshape(gradients_abs_gen[k], [-1]) for k in grad_key_gen], axis=0) 
    normalgrad_v_gen = tf.divide(grad_v_gen, tf.reduce_sum(grad_v_gen))
    num_params_gen = normalgrad_v_gen.shape.as_list()[0]
    kappa_gen = int(round(num_params_gen * (1. - target_sparsity)))   
    topk_gen, ind_gen = tf.nn.top_k(normalgrad_v_gen, k=kappa_gen, sorted=True)
    sp_mask_gen = tf.SparseTensor(dense_shape=normalgrad_v_gen.shape.as_list(), values=tf.ones_like(ind_gen, dtype=tf.float32).numpy(), indices=np.expand_dims(ind_gen.numpy(), 1))
    mask_v_gen = tf.sparse.to_dense(sp_mask_gen, validate_indices=False)
    #restore mask_v as dictionary of weights
    v_splits_gen = tf.split(mask_v_gen, num_or_size_splits=split_sizes_gen)
    mask_restore_gen = {}
    for i, key in enumerate(grad_key_gen):
       mask_restore_gen.update({key: tf.reshape(v_splits_gen[i], grad_shape_gen[key])})
    final_mask_gen = mask_restore_gen
    final_w_gen = apply_mask(weights_gen, final_mask_gen)

    return final_w_disc, final_mask_disc, final_w_gen, final_mask_gen



    
                    
        